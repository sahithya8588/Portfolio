<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Sahithya.E - Data Engineer Portfolio</title>
  <link rel="stylesheet" href="styles.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <link href="https://unpkg.com/aos@2.3.1/dist/aos.css" rel="stylesheet">
  <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
</head>
<body data-theme="light">
  <nav>
    <div class="logo">
      <svg width="40" height="40" viewBox="0 0 40 40">
        <circle cx="20" cy="20" r="18" fill="none" stroke="currentColor" stroke-width="2"/>
        <text x="50%" y="50%" text-anchor="middle" dy=".3em" fill="currentColor">SE</text>
      </svg>
      <span>Sahithya.E</span>
    </div>
    <div class="nav-links">
      <a href="#home">Home</a>
      <a href="#about">About</a>
      <a href="#skills">Skills</a>
      <a href="#work">Experience</a>
      <a href="#education">Education</a>
      <a href="#contact">Contact</a>
    </div>
    <button class="theme-toggle">
      <i class="fas fa-moon"></i>
    </button>
  </nav>

  <section id="home">
    <div class="hero" data-aos="fade-up">
      <div class="hero-content">
        <div class="profile-header">
          <div class="profile-image">
            <svg class="profile-placeholder" viewBox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
              <!-- Background Circle -->
              <circle cx="50" cy="50" r="45" fill="var(--primary-color)" opacity="0.1"/>
              <!-- Embedded Image -->
              <image href="MPIC.jpg" x="5" y="5" width="90" height="90" clip-path="circle(50% at 50% 50%)"/>
            </svg>
            <div class="upload-overlay">
              <i class="fas fa-camera"></i>
              <input type="file" id="profile-upload" accept="image/*">
            </div>
          </div>
          <div class="profile-text">
            <h1>Hello, I'm <span class="highlight">Sahithya.E</span></h1>
            <h2>Senior Data Engineer</h2>
            <p>Transforming Complex Data Challenges into Business Solutions</p>
          </div>
        </div>
        <div class="cta-buttons">
          <a href="#contact" class="primary-btn">Get in Touch</a>
          <a href="#work" class="secondary-btn">View Experience</a>
        </div>
      </div>
      <div class="hero-image">
        <div class="data-viz">
          <canvas id="skillChart"></canvas>
        </div>
      </div>
    </div>
  </section>

  <section id="about" data-aos="fade-up">
    <h2>About me</h2>
    <div class="about-content">
      <div class="about-text">
        <p>
        I am a senior Data Engineer with over 7+ years of experience in designing, building, and optimizing scalable data pipelines and ETL workflows across financial, healthcare, and retail industries. My expertise spans cloud technologies (AWS, Azure, Snowflake) and big data frameworks (PySpark, Hadoop, Kafka), ensuring high-performance, secure, and cost-effective data solutions.

With a deep understanding of cloud data architectures, I have successfully implemented large-scale ETL pipelines using AWS Glue and PySpark, managed data lakes on S3, and automated real-time data ingestion with AWS Lambda and Step Functions. My experience includes migrating databases to AWS with DMS and optimizing streaming solutions using AWS Kinesis for real-time analytics.

In Azure, I have designed and deployed scalable data workflows using Azure Data Factory, Azure SQL, and Azure Machine Learning Studio, ensuring compliance with GDPR, HIPAA, and PCI-DSS regulations. I specialize in data modeling, utilizing Star and Snowflake schemas to enhance query performance and reduce ETL processing times by 50%.

My expertise extends to Snowflake integration with BI tools like Tableau and Power BI, supporting real-time analytics and business intelligence. I also have hands-on experience deploying CI/CD pipelines using Terraform, Jenkins, and Kubernetes, ensuring automated and reliable data infrastructure.

Passionate about solving complex data challenges, I enjoy working at the intersection of data engineering and data science, collaborating with analysts and scientists to enable predictive analytics with Pandas, Scikit-learn, and ML frameworks. My goal is to build efficient, scalable, and innovative data solutions that drive business growth and operational excellence.</p>
      </div>
    </div>
  </section>

  <section id="skills" data-aos="fade-up">
    <h2>Technical Expertise</h2>
    <div class="skills-container">
      <div class="skill-category">
        <h3>Programming & Databases</h3>
        <div class="skill-list">
          <div class="skill-item">
            <i class="fas fa-code"></i>
            <span>SQL, PL/SQL</span>
          </div>
          <div class="skill-item">
            <i class="fab fa-python"></i>
            <span>Python, R</span>
          </div>
          <div class="skill-item">
            <i class="fas fa-database"></i>
            <span>MySQL, MS SQL</span>
          </div>
          <div class="skill-item">
            <i class="fas fa-server"></i>
            <span>Teradata, Hive</span>
          </div>
        </div>
      </div>
      
      <div class="skill-category">
        <h3>Cloud & ETL Tools</h3>
        <div class="skill-list">
          <div class="skill-item">
            <i class="fab fa-aws"></i>
            <span>AWS Suite</span>
          </div>
          <div class="skill-item">
            <i class="fab fa-microsoft"></i>
            <span>Azure</span>
          </div>
          <div class="skill-item">
            <i class="fas fa-tools"></i>
            <span>Informatica</span>
          </div>
          <div class="skill-item">
            <i class="fas fa-fire"></i>
            <span>Pentaho</span>
          </div>
        </div>
      </div>
      
      <div class="skill-category">
        <h3>Visualization & Tools</h3>
        <div class="skill-list">
          <div class="skill-item">
            <i class="fas fa-chart-bar"></i>
            <span>Power BI</span>
          </div>
          <div class="skill-item">
            <i class="fas fa-chart-line"></i>
            <span>Tableau</span>
          </div>
          <div class="skill-item">
            <i class="fab fa-windows"></i>
            <span>Visual Studio</span>
          </div>
          <div class="skill-item">
            <i class="fab fa-linux"></i>
            <span>Linux/Windows</span>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section id="work" data-aos="fade-up">
    <h2>Recent Work</h2>
    <div class="experience-timeline">
      <div class="experience-card active">
        <div class="experience-header">
          <h3>Home Depot</h3>
          <span class="date">Jan 2024 - Present</span>
        </div>
        <div class="experience-content">
          <ul>
          <li>Designed and implemented complex ETL pipelines using PySpark to integrate diverse data sources into Snowflake, AWS Data Lake (S3), and Hive, ensuring data quality, consistency, and scalability for enterprise-wide analytics and reporting.</li>
          <li>Optimized data processing workflows in AWS and Snowflake, leveraging tools like PySpark, AWS Glue, and Snowflake queries to enhance performance and support large-scale data analysis, predictive modeling, and real-time analytics.</li>
          <li>Automated ETL workflows and data transformations using PySpark and AWS Glue, reducing manual intervention, ensuring timely data updates, and enhancing data-driven decision-making across business functions.</li>
          <li>Deployed AWS data governance frameworks such as Collibra for metadata management, data lineage, and cataloguing, ensuring visibility, compliance, and security of critical business data in Snowflake and AWS environments.</li>
          <li>Collaborated with data scientists to develop machine learning models using AWS SageMaker and Python, enabling predictive analytics and actionable business insights to optimize retail operations and decision-making processes.</li>
          <li>Enhanced data security and compliance by implementing encryption, access control, and data protection measures in AWS and Snowflake, safeguarding sensitive operational and business intelligence data.</li>
          <li>Developed and maintained Python-based data pipelines, AWS Lambda, and Step Functions to automate data ingestion, transformation, and processing, improving operational efficiency and enabling consistent access to analytics-ready datasets.</li>
          </ul>
          <div class="tech-stack">
            <span class="tech-badge">PySpark</span>
            <span class="tech-badge">Snowflake</span>
            <span class="tech-badge">AWS Glue</span>
          </div>
        </div>
      </div>

      <div class="experience-card">
        <div class="experience-header">
          <h3>National Grid</h3>
          <span class="date">Oct 2021 - Dec 2023</span>
        </div>
        <div class="experience-content">
          <ul>
            <li>Migrated on-premises and Azure-based data solutions to AWS, leveraging AWS-native services like S3, Glue, and Kinesis to enhance scalability, performance, and cost optimization.</li>
            <li>Transitioned data storage from Azure Blob Storage and Azure Data Lake Storage to Amazon S3, implementing lifecycle policies, versioning, and secure encryption via AWS KMS, ensuring data integrity and regulatory compliance.</li>
            <li>Reengineered ETL workflows by migrating from Azure Data Factory to AWS Glue and AWS Step Functions, automating processes, and improving processing efficiency for financial data.</li>
            <li>Migrated real-time data processing solutions from Azure Event Hubs and Stream Analytics to AWS Kinesis and Lambda, achieving low-latency processing for financial transactions.</li>
            <li>Replaced Azure SQL Database and Synapse Analytics with AWS Redshift and Athena for improved query performance, advanced analytics, and cost-effective big data processing.</li>
            <li>Implemented monitoring, alerting, and access control systems by transitioning from Azure Monitor, RBAC, and Power BI to AWS CloudWatch, IAM, and QuickSight for seamless pipeline performance tracking and business insights.</li>
            <li>Replaced Azure-based services like Logic Apps, Functions, and DevOps pipelines with AWS Lambda, EventBridge, S3, and CodePipeline, ensuring scalability, automation, and streamlined CI/CD for data workflows.</li>
          </ul>
          <div class="tech-stack">
            <span class="tech-badge">Azure</span>
            <span class="tech-badge">AWS</span>
            <span class="tech-badge">Data Pipeline</span>
          </div>
        </div>
      </div>

      <div class="experience-card">
        <div class="experience-header">
          <h3>GAP</h3>
          <span class="date">Mar 2018 - Sep 2021</span>
        </div>
        <div class="experience-content">
          <ul>
            <li>Design and implement data pipelines to transfer data from various sources to Azure destinations, such as Azure Data Lake, Azure SQL Database, or Azure Synapse Analytics.</li>
            <li>Build and optimize data storage solutions on Azure, leveraging Azure Blob Storage, Azure Data Lake Storage, or Azure SQL Database to efficiently handle large volumes of data.</li>
            <li>Develop and maintain data processing solutions on Azure using Azure Databricks, Azure HDInsight, or Azure Stream Analytics to transform, cleanse, and enrich data.</li>
            <li>Monitor the performance of Azure data solutions, troubleshoot issues, and resolve bottlenecks, data quality concerns, and other performance-related problems.</li>
            <li>Develop and maintain BI applications, dashboards, and reports using Microsoft Power BI, ensuring data accuracy, integrity, and security within the BI environment.</li>
            <li>Collaborate with cross-functional teams to define KPI</li>
          </ul>
          <div class="tech-stack">
            <span class="tech-badge">Azure Data Factory</span>
            <span class="tech-badge">Power BI</span>
            <span class="tech-badge">SQL Server</span>
          </div>
        </div>
      </div>

      <div class="experience-card">
        <div class="experience-header">
          <h3>Paytm</h3>
          <span class="date">Mar 2017 - Feb 2018</span>
        </div>
        <div class="experience-content">
          <ul>
            <li>Designed and managed a centralized data lake on Amazon S3, providing scalable and secure storage for financial data.</li>  
            <li>Automated data ingestion and processing workflows using AWS Lambda, ensuring timely updates and reducing manual intervention.</li>  
            <li>Implemented ETL processes using AWS Glue to migrate and transform financial data from various sources into Snowflake.</li>  
            <li>Utilized AWS Kinesis for real-time data streaming, enabling timely financial transactions and fraud detection.</li>  
            <li>Configured and managed data security using AWS IAM policies and AWS Key Management Service (KMS) to ensure compliance with financial regulations.</li>  
            <li>Implemented monitoring and logging for data pipelines using AWS CloudWatch, improving reliability and facilitating quick issue resolution.</li> 
            <li>Leveraged AWS Redshift for data warehousing solutions, supporting comprehensive financial analytics and reporting.</li>
          </ul>
          <div class="tech-stack">
            <span class="tech-badge">AWS</span>
            <span class="tech-badge">Python</span>
            <span class="tech-badge">Data Lakes</span>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section id="education" data-aos="fade-up">
    <h2>Education</h2>
    <div class="education-timeline">
      <div class="timeline-item">
        <div class="timeline-content">
          <h3>Master of Science in Computer Science</h3>
          <p>University of Memphis, USA</p>
          <div class="education-details">
            <i class="fas fa-graduation-cap"></i>
            <span>Advanced coursework in Data Engineering and Analytics</span>
          </div>
        </div>
      </div>
      <div class="timeline-item">
        <div class="timeline-content">
          <h3>Bachelor in Electronics and Communication Engineering</h3>
          <p>Krishnaveni Engineering College, India</p>
          <div class="education-details">
            <i class="fas fa-university"></i>
            <span>Foundation in Technical Sciences</span>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section id="contact" data-aos="fade-up">
    <h2>Get in Touch</h2>
    <div class="contact-container">
     
      <div class="contact-info">
        <div class="contact-item">
          <i class="fas fa-envelope"></i>
          <a href="mailto:sahithya.e@example.com">sahithya8588@gmail.com</a>
        </div>
        <div class="contact-item">
          <i class="fas fa-phone"></i>
          <a href="tel:+1234567890">+1 (901)4921215</a>
        </div>
        <div class="contact-item">
          <i class="fas fa-map-marker-alt"></i>
          <span>Memphis, Tennessee</span>
        </div>
          </a>
        </div>
      </div>
    </div>
  </section>

  <footer>
    <div class="footer-content">
      <div class="footer-links">
        <a href="#home">Home</a>
        <a href="#about">About</a>
        <a href="#skills">Skills</a>
        <a href="#work">Experience</a>
        <a href="#contact">Contact</a>
      </div>
      <p>&copy; 2024 Sahithya.E - Data Engineering Portfolio. All rights reserved.</p>
    </div>
  </footer>

  <script src="https://unpkg.com/aos@2.3.1/dist/aos.js"></script>
  <script src="script.js"></script>
</body>
</html>
